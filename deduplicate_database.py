#!/usr/bin/env python3
"""
Supabaseæ•°æ®åº“å»é‡è„šæœ¬
ç”¨äºæ¸…ç†ç°æœ‰æ•°æ®åº“ä¸­çš„é‡å¤æ–‡ç« 
"""
import json
import logging
from datetime import datetime
from typing import List, Dict, Set, Tuple
from techcrunch_crawler import TechCrunchCrawler, load_config

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class DatabaseDeduplicator:
    def __init__(self, supabase_config: Dict):
        self.supabase_config = supabase_config
        self.crawler = TechCrunchCrawler(supabase_config)
        if not self.crawler.supabase_client:
            raise Exception("Supabaseå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥")
        
        self.table_name = supabase_config.get('table_name', 'news_items')
        
    def _is_text_similar(self, text1: str, text2: str, threshold: float = 0.85) -> bool:
        """æ£€æŸ¥ä¸¤æ®µæ–‡æœ¬æ˜¯å¦ç›¸ä¼¼ï¼ˆåŸºäºå­—ç¬¦çº§ç›¸ä¼¼åº¦ï¼‰"""
        if not text1 or not text2:
            return False
        
        # å¦‚æœæ–‡æœ¬å®Œå…¨ç›¸åŒ
        if text1 == text2:
            return True
        
        # å¦‚æœé•¿åº¦å·®å¼‚å¤ªå¤§ï¼Œè®¤ä¸ºä¸ç›¸ä¼¼
        len_diff = abs(len(text1) - len(text2))
        max_len = max(len(text1), len(text2))
        if max_len > 0 and len_diff / max_len > 0.3:
            return False
        
        # ä½¿ç”¨å­—ç¬¦çº§åˆ«çš„n-gramè®¡ç®—ç›¸ä¼¼åº¦
        def get_char_ngrams(text, n=3):
            if len(text) < n:
                return {text}
            return set(text[i:i+n] for i in range(len(text)-n+1))
        
        ngrams1 = get_char_ngrams(text1, 3)
        ngrams2 = get_char_ngrams(text2, 3)
        
        if not ngrams1 or not ngrams2:
            return False
        
        intersection = len(ngrams1.intersection(ngrams2))
        union = len(ngrams1.union(ngrams2))
        
        if union == 0:
            return False
        
        similarity = intersection / union
        return similarity >= threshold
    
    def get_all_articles(self) -> List[Dict]:
        """è·å–æ•°æ®åº“ä¸­æ‰€æœ‰æ–‡ç« """
        try:
            logging.info("æ­£åœ¨è·å–æ•°æ®åº“ä¸­çš„æ‰€æœ‰æ–‡ç« ...")
            result = self.crawler.supabase_client.table(self.table_name).select("*").execute()
            articles = result.data
            logging.info(f"è·å–åˆ° {len(articles)} ç¯‡æ–‡ç« ")
            return articles
        except Exception as e:
            logging.error(f"è·å–æ–‡ç« å¤±è´¥: {e}")
            return []
    
    def find_duplicates(self, articles: List[Dict]) -> List[Tuple[Dict, List[Dict]]]:
        """
        æŸ¥æ‰¾é‡å¤æ–‡ç« 
        è¿”å›æ ¼å¼: [(ä¿ç•™çš„æ–‡ç« , [é‡å¤æ–‡ç« åˆ—è¡¨]), ...]
        """
        logging.info("å¼€å§‹æŸ¥æ‰¾é‡å¤æ–‡ç« ...")
        
        duplicates = []
        processed_ids = set()
        
        for i, article in enumerate(articles):
            if article['id'] in processed_ids:
                continue
            
            article_id = article['id']
            title = article.get('title', '')
            content = article.get('content', '')
            url = article.get('url', '')
            
            # æå–å†…å®¹å‰ç¼€ç”¨äºæ¯”è¾ƒ
            content_prefix = ""
            if content:
                content_prefix = content[:300].lower().strip()
                content_prefix = ' '.join(content_prefix.split())
            
            # æŸ¥æ‰¾ç›¸ä¼¼æ–‡ç« 
            similar_articles = []
            
            for j, other_article in enumerate(articles[i+1:], i+1):
                if other_article['id'] in processed_ids:
                    continue
                
                other_title = other_article.get('title', '')
                other_content = other_article.get('content', '')
                other_url = other_article.get('url', '')
                
                is_duplicate = False
                duplicate_reason = ""
                
                # 1. URLå®Œå…¨åŒ¹é…
                if url and other_url and url == other_url:
                    is_duplicate = True
                    duplicate_reason = "URLç›¸åŒ"
                
                # 2. å†…å®¹å‰ç¼€ç›¸ä¼¼æ€§æ£€æŸ¥
                elif content and other_content:
                    other_content_prefix = other_content[:300].lower().strip()
                    other_content_prefix = ' '.join(other_content_prefix.split())
                    
                    if len(content_prefix) > 50 and len(other_content_prefix) > 50:
                        if self._is_text_similar(content_prefix, other_content_prefix):
                            is_duplicate = True
                            duplicate_reason = "å†…å®¹ç›¸ä¼¼"
                
                if is_duplicate:
                    similar_articles.append({
                        'article': other_article,
                        'reason': duplicate_reason
                    })
                    processed_ids.add(other_article['id'])
                    logging.info(f"å‘ç°é‡å¤: '{title}' vs '{other_title}' ({duplicate_reason})")
            
            if similar_articles:
                duplicates.append((article, similar_articles))
                processed_ids.add(article_id)
        
        logging.info(f"æ‰¾åˆ° {len(duplicates)} ç»„é‡å¤æ–‡ç« ")
        return duplicates
    
    def preview_duplicates(self, duplicates: List[Tuple[Dict, List[Dict]]]):
        """é¢„è§ˆè¦åˆ é™¤çš„é‡å¤æ–‡ç« """
        if not duplicates:
            logging.info("æ²¡æœ‰å‘ç°é‡å¤æ–‡ç« ")
            return
        
        total_to_delete = 0
        print("\n" + "="*80)
        print("é‡å¤æ–‡ç« é¢„è§ˆ")
        print("="*80)
        
        for i, (keep_article, duplicate_list) in enumerate(duplicates, 1):
            print(f"\nã€ç¬¬{i}ç»„é‡å¤ã€‘")
            print(f"ä¿ç•™: {keep_article.get('title', 'N/A')}")
            print(f"     ID: {keep_article['id']}")
            print(f"     å‘å¸ƒæ—¶é—´: {keep_article.get('published_at', 'N/A')}")
            print(f"     URL: {keep_article.get('url', 'N/A')}")
            
            print(f"\nå°†åˆ é™¤ {len(duplicate_list)} ç¯‡é‡å¤æ–‡ç« :")
            for j, dup_info in enumerate(duplicate_list, 1):
                dup_article = dup_info['article']
                reason = dup_info['reason']
                print(f"  {j}. [{reason}] {dup_article.get('title', 'N/A')}")
                print(f"     ID: {dup_article['id']} | å‘å¸ƒæ—¶é—´: {dup_article.get('published_at', 'N/A')}")
                total_to_delete += 1
        
        print(f"\næ€»è®¡: å‘ç° {len(duplicates)} ç»„é‡å¤ï¼Œå°†åˆ é™¤ {total_to_delete} ç¯‡æ–‡ç« ")
        print("="*80)
    
    def delete_duplicates(self, duplicates: List[Tuple[Dict, List[Dict]]], dry_run: bool = True) -> int:
        """åˆ é™¤é‡å¤æ–‡ç« """
        if not duplicates:
            logging.info("æ²¡æœ‰é‡å¤æ–‡ç« éœ€è¦åˆ é™¤")
            return 0
        
        to_delete_ids = []
        for keep_article, duplicate_list in duplicates:
            for dup_info in duplicate_list:
                to_delete_ids.append(dup_info['article']['id'])
        
        if dry_run:
            logging.info(f"[é¢„æ¼”æ¨¡å¼] å°†åˆ é™¤ {len(to_delete_ids)} ç¯‡é‡å¤æ–‡ç« ")
            return len(to_delete_ids)
        
        deleted_count = 0
        
        # åˆ†æ‰¹åˆ é™¤
        batch_size = 10
        for i in range(0, len(to_delete_ids), batch_size):
            batch_ids = to_delete_ids[i:i + batch_size]
            
            try:
                # ä½¿ç”¨inæ“ä½œç¬¦åˆ é™¤å¤šä¸ªè®°å½•
                result = self.crawler.supabase_client.table(self.table_name).delete().in_('id', batch_ids).execute()
                batch_deleted = len(batch_ids)
                deleted_count += batch_deleted
                logging.info(f"å·²åˆ é™¤æ‰¹æ¬¡ {i//batch_size + 1}: {batch_deleted} ç¯‡æ–‡ç« ")
            except Exception as e:
                logging.error(f"åˆ é™¤æ‰¹æ¬¡ {i//batch_size + 1} å¤±è´¥: {e}")
        
        logging.info(f"âœ… æ€»å…±åˆ é™¤äº† {deleted_count} ç¯‡é‡å¤æ–‡ç« ")
        return deleted_count
    
    def create_backup(self, articles: List[Dict]) -> str:
        """åˆ›å»ºæ•°æ®å¤‡ä»½"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_file = f"database_backup_{timestamp}.json"
        
        try:
            with open(backup_file, 'w', encoding='utf-8') as f:
                json.dump(articles, f, ensure_ascii=False, indent=2, default=str)
            
            logging.info(f"æ•°æ®å¤‡ä»½å·²ä¿å­˜: {backup_file}")
            return backup_file
        except Exception as e:
            logging.error(f"åˆ›å»ºå¤‡ä»½å¤±è´¥: {e}")
            return None

def main():
    """ä¸»å‡½æ•°"""
    print("Supabase æ•°æ®åº“å»é‡å·¥å…·")
    print("="*50)
    
    # åŠ è½½é…ç½®
    config = load_config()
    if not config:
        print("âŒ æœªæ‰¾åˆ°Supabaseé…ç½®")
        return
    
    try:
        # åˆå§‹åŒ–å»é‡å™¨
        deduplicator = DatabaseDeduplicator(config)
        
        # è·å–æ‰€æœ‰æ–‡ç« 
        articles = deduplicator.get_all_articles()
        if not articles:
            print("âŒ æ— æ³•è·å–æ–‡ç« æ•°æ®")
            return
        
        # åˆ›å»ºå¤‡ä»½
        print(f"\nğŸ“¦ æ­£åœ¨åˆ›å»ºæ•°æ®å¤‡ä»½...")
        backup_file = deduplicator.create_backup(articles)
        if not backup_file:
            print("âš ï¸ å¤‡ä»½å¤±è´¥ï¼Œä½†å°†ç»§ç»­æ‰§è¡Œ")
        
        # æŸ¥æ‰¾é‡å¤
        duplicates = deduplicator.find_duplicates(articles)
        
        # é¢„è§ˆé‡å¤æ–‡ç« 
        deduplicator.preview_duplicates(duplicates)
        
        if not duplicates:
            print("âœ… æ•°æ®åº“ä¸­æ²¡æœ‰é‡å¤æ–‡ç« ")
            return
        
        # è¯¢é—®ç”¨æˆ·æ˜¯å¦æ‰§è¡Œåˆ é™¤
        print(f"\nâš ï¸ æ³¨æ„ï¼šæ­¤æ“ä½œå°†æ°¸ä¹…åˆ é™¤é‡å¤æ–‡ç« ï¼")
        if backup_file:
            print(f"ğŸ“¦ æ•°æ®å¤‡ä»½å·²ä¿å­˜è‡³: {backup_file}")
        
        choice = input("\næ˜¯å¦æ‰§è¡Œåˆ é™¤æ“ä½œï¼Ÿ(y/N): ").strip().lower()
        
        if choice == 'y':
            print("ğŸ—‘ï¸ æ­£åœ¨åˆ é™¤é‡å¤æ–‡ç« ...")
            deleted_count = deduplicator.delete_duplicates(duplicates, dry_run=False)
            print(f"âœ… åˆ é™¤å®Œæˆï¼å…±åˆ é™¤ {deleted_count} ç¯‡é‡å¤æ–‡ç« ")
            
            # éªŒè¯ç»“æœ
            remaining_articles = deduplicator.get_all_articles()
            print(f"ğŸ“Š æ•°æ®åº“ç°æœ‰æ–‡ç« æ•°: {len(remaining_articles)}")
            
        else:
            print("âŒ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
            
    except Exception as e:
        logging.error(f"è¿è¡Œå‡ºé”™: {e}", exc_info=True)

if __name__ == "__main__":
    main()